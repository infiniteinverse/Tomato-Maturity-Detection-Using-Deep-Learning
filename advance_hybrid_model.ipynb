{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35332,"status":"ok","timestamp":1745458945447,"user":{"displayName":"Unicorna","userId":"15069696722901594959"},"user_tz":-330},"id":"JbnXPXMy7JDr","outputId":"7b678529-91ad-417d-cf08-5489e01c299f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["['Tomato_5_class']"]},"metadata":{},"execution_count":1}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import zipfile\n","import os\n","\n","zip_path = '/content/drive/MyDrive/Tomato_5_class.zip'  # Update this path if needed\n","extract_path = '/content/Tomato_5_class'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","os.listdir(extract_path)  # Check contents"]},{"cell_type":"markdown","metadata":{"id":"rzYkjMSq79U6"},"source":["model"]},{"cell_type":"code","source":["# 🚩 Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from transformers import ViTForImageClassification\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import numpy as np\n","\n","# ✅ Config\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Paths\n","data_dir = \"/content/drive/MyDrive/Tomato Maturity Detection Dataset/Augment Dataset\"\n","save_path = \"/content/drive/MyDrive/2classbestmodel.pth\"\n","\n","# 🔁 Transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","])\n","\n","# 📦 Dataset & Dataloader\n","full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","class_names = full_dataset.classes  # ['IMMATURE', 'MATURE']\n","print(\"Classes:\", class_names)\n","\n","from torch.utils.data import random_split\n","train_size = int(0.8 * len(full_dataset))\n","val_size = len(full_dataset) - train_size\n","train_data, val_data = random_split(full_dataset, [train_size, val_size])\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n","\n","# ✅ Model Setup\n","model = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224-in21k\",\n","    num_labels=2  # 2 classes: MATURE / IMMATURE\n",")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 🔁 Training & Evaluation Functions\n","def train_one_epoch(model, loader, optimizer, criterion):\n","    model.train()\n","    total_loss, correct, total = 0, 0, 0\n","    for imgs, labels in tqdm(loader, desc=\"Training\"):\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        outputs = model(imgs).logits\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        preds = outputs.argmax(dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","    return total_loss / len(loader), correct / total\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss, correct, total = 0, 0, 0\n","    with torch.no_grad():\n","        for imgs, labels in tqdm(loader, desc=\"Evaluating\"):\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            outputs = model(imgs).logits\n","            loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item()\n","            preds = outputs.argmax(dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","    return total_loss / len(loader), correct / total\n","\n","# 🧠 Training Loop\n","EPOCHS = 5\n","train_losses, val_losses = [], []\n","train_accuracies, val_accuracies = [], []\n","best_val_acc = 0.0\n","\n","for epoch in range(EPOCHS):\n","    print(f\"\\n📚 Epoch {epoch+1}/{EPOCHS}\")\n","    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n","    val_loss, val_acc = evaluate(model, val_loader, criterion)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    train_accuracies.append(train_acc)\n","    val_accuracies.append(val_acc)\n","\n","    print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n","    print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"✅ Best model saved to Google Drive at: {save_path}\")\n","\n","# 📈 Plot Learning Curves\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_accuracies, label='Train Acc')\n","plt.plot(val_accuracies, label='Val Acc')\n","plt.title(\"Accuracy\")\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Val Loss')\n","plt.title(\"Loss\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"JagEHAsRL4cw","executionInfo":{"status":"error","timestamp":1745471183236,"user_tz":-330,"elapsed":53170,"user":{"displayName":"Unicorna","userId":"15069696722901594959"}},"outputId":"e5ca0404-a6de-4040-937b-bb78dfee36c8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Using device: cuda\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Tomato Maturity Detection Dataset/Augment Dataset'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6d263b401a61>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 📦 Dataset & Dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m  \u001b[0;31m# ['IMMATURE', 'MATURE']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Tomato Maturity Detection Dataset/Augment Dataset'"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VopJGJ77_R5","outputId":"21466340-787d-4598-d6d6-1518a9089c24","executionInfo":{"status":"ok","timestamp":1745462935760,"user_tz":-330,"elapsed":1797699,"user":{"displayName":"Unicorna","userId":"15069696722901594959"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5122 images belonging to 5 classes.\n","Found 1275 images belonging to 5 classes.\n","Phase 1: Training with frozen backbone...\n","Epoch 1/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 472ms/step - accuracy: 0.3144 - loss: 0.3937 - val_accuracy: 0.6149 - val_loss: 0.1495 - learning_rate: 1.0000e-04\n","Epoch 2/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 295ms/step - accuracy: 0.4242 - loss: 0.2893 - val_accuracy: 0.6282 - val_loss: 0.1396 - learning_rate: 1.0000e-04\n","Epoch 3/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 274ms/step - accuracy: 0.4774 - loss: 0.2573 - val_accuracy: 0.6275 - val_loss: 0.1358 - learning_rate: 1.0000e-04\n","Epoch 4/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 273ms/step - accuracy: 0.4873 - loss: 0.2445 - val_accuracy: 0.6196 - val_loss: 0.1362 - learning_rate: 1.0000e-04\n","Epoch 5/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 279ms/step - accuracy: 0.5221 - loss: 0.2205 - val_accuracy: 0.6282 - val_loss: 0.1335 - learning_rate: 1.0000e-04\n","Epoch 6/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 283ms/step - accuracy: 0.5198 - loss: 0.2127 - val_accuracy: 0.6306 - val_loss: 0.1303 - learning_rate: 1.0000e-04\n","Epoch 7/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 288ms/step - accuracy: 0.5205 - loss: 0.2135 - val_accuracy: 0.6275 - val_loss: 0.1292 - learning_rate: 1.0000e-04\n","Epoch 8/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 276ms/step - accuracy: 0.5308 - loss: 0.2016 - val_accuracy: 0.6133 - val_loss: 0.1319 - learning_rate: 1.0000e-04\n","Epoch 9/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 272ms/step - accuracy: 0.5377 - loss: 0.1866 - val_accuracy: 0.6259 - val_loss: 0.1280 - learning_rate: 1.0000e-04\n","Epoch 10/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 280ms/step - accuracy: 0.5345 - loss: 0.1923 - val_accuracy: 0.6369 - val_loss: 0.1243 - learning_rate: 1.0000e-04\n","Epoch 11/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 283ms/step - accuracy: 0.5502 - loss: 0.1824 - val_accuracy: 0.6431 - val_loss: 0.1238 - learning_rate: 1.0000e-04\n","Epoch 12/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 281ms/step - accuracy: 0.5527 - loss: 0.1777 - val_accuracy: 0.6447 - val_loss: 0.1249 - learning_rate: 1.0000e-04\n","Epoch 13/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 287ms/step - accuracy: 0.5623 - loss: 0.1713 - val_accuracy: 0.6384 - val_loss: 0.1236 - learning_rate: 1.0000e-04\n","Epoch 14/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 276ms/step - accuracy: 0.5508 - loss: 0.1709 - val_accuracy: 0.6361 - val_loss: 0.1250 - learning_rate: 1.0000e-04\n","Epoch 15/15\n","\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 274ms/step - accuracy: 0.5671 - loss: 0.1683 - val_accuracy: 0.6220 - val_loss: 0.1221 - learning_rate: 1.0000e-04\n","Unfreezing EfficientNet layers for fine-tuning...\n","Unfreezing DenseNet layers for fine-tuning...\n","Phase 2: Fine-tuning...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 58 variables. \n","  saveable.load_own_variables(weights_store.get(inner_path))\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 227ms/step - accuracy: 0.8717 - loss: 0.0564\n","\n","Final Test Accuracy: 0.6447\n","Final Test Loss: 0.1249\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models, optimizers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","import numpy as np\n","\n","# Set mixed precision policy\n","tf.keras.mixed_precision.set_global_policy('float32')\n","\n","# Dataset paths\n","train_path = \"/content/Tomato_5_class/Tomato_5_class/Training_set\"\n","test_path = \"/content/Tomato_5_class/Tomato_5_class/Testing_set\"\n","\n","# Enhanced data augmentation for plant disease classification\n","train_datagen = ImageDataGenerator(\n","    rotation_range=45,\n","    width_shift_range=0.3,\n","    height_shift_range=0.3,\n","    shear_range=0.3,\n","    zoom_range=[0.7, 1.3],\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    brightness_range=[0.7, 1.3],\n","    fill_mode='reflect',\n","    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",")\n","\n","test_datagen = ImageDataGenerator(\n","    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",")\n","\n","# Data generators\n","train_gen = train_datagen.flow_from_directory(\n","    train_path,\n","    target_size=(224, 224),\n","    batch_size=16,\n","    class_mode='sparse',\n","    shuffle=True\n",")\n","\n","test_gen = test_datagen.flow_from_directory(\n","    test_path,\n","    target_size=(224, 224),\n","    batch_size=16,\n","    class_mode='sparse',\n","    shuffle=False\n",")\n","\n","# Custom attention layer\n","class SelfAttention(layers.Layer):\n","    def __init__(self, embedding_dim):\n","        super(SelfAttention, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.query = layers.Dense(embedding_dim)\n","        self.key = layers.Dense(embedding_dim)\n","        self.value = layers.Dense(embedding_dim)\n","\n","    def call(self, inputs):\n","        # Get query, key, value projections\n","        query = self.query(inputs)\n","        key = self.key(inputs)\n","        value = self.value(inputs)\n","\n","        # Reshape for matrix multiplication\n","        batch_size = tf.shape(inputs)[0]\n","\n","        # Using einsum for cleaner matrix multiplication\n","        score = tf.einsum('bij,bkj->bik', query, key)\n","\n","        # Scale\n","        scale = tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n","        scaled_score = score / scale\n","\n","        # Softmax for attention weights\n","        attention_weights = tf.nn.softmax(scaled_score, axis=-1)\n","\n","        # Apply attention to values\n","        context = tf.einsum('bij,bjk->bik', attention_weights, value)\n","\n","        return context\n","\n","# Enhanced model without complex transformer parts\n","def create_enhanced_model(num_classes=5):\n","    # Input layer\n","    inputs = layers.Input(shape=(224, 224, 3))\n","\n","    # CNN Branch 1: EfficientNetB2\n","    effnet = tf.keras.applications.EfficientNetB2(\n","        include_top=False,\n","        weights='imagenet',\n","        input_tensor=inputs\n","    )\n","\n","    # Initially freeze all layers\n","    for layer in effnet.layers:\n","        layer.trainable = False\n","\n","    effnet_features = layers.GlobalAveragePooling2D()(effnet.output)\n","\n","    # CNN Branch 2: DenseNet121 (good alternative with different architecture)\n","    densenet = tf.keras.applications.DenseNet121(\n","        include_top=False,\n","        weights='imagenet',\n","        input_tensor=inputs\n","    )\n","\n","    # Initially freeze all layers\n","    for layer in densenet.layers:\n","        layer.trainable = False\n","\n","    densenet_features = layers.GlobalAveragePooling2D()(densenet.output)\n","\n","    # Feature processing with dedicated heads\n","    effnet_features = layers.Dense(512, activation='swish')(effnet_features)\n","    effnet_features = layers.BatchNormalization()(effnet_features)\n","    effnet_features = layers.Dropout(0.3)(effnet_features)\n","\n","    densenet_features = layers.Dense(512, activation='swish')(densenet_features)\n","    densenet_features = layers.BatchNormalization()(densenet_features)\n","    densenet_features = layers.Dropout(0.3)(densenet_features)\n","\n","    # Feature fusion\n","    combined = layers.Concatenate()([effnet_features, densenet_features])\n","\n","    # Reshape for self-attention\n","    combined = layers.Reshape((1, 1024))(combined)\n","\n","    # Apply attention - using our proper Keras attention layer\n","    attention_output = SelfAttention(1024)(combined)\n","\n","    # Flatten\n","    features = layers.Flatten()(attention_output)\n","\n","    # Deep classifier\n","    x = layers.Dense(768, activation='swish')(features)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dropout(0.4)(x)\n","    x = layers.Dense(384, activation='swish')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dropout(0.3)(x)\n","    x = layers.Dense(192, activation='swish')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dropout(0.2)(x)\n","\n","    # Output\n","    outputs = layers.Dense(num_classes, activation='softmax')(x)\n","\n","    return models.Model(inputs=inputs, outputs=outputs)\n","\n","# Focal Loss\n","class FocalLoss(tf.keras.losses.Loss):\n","    def __init__(self, alpha=0.25, gamma=2.0):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def call(self, y_true, y_pred):\n","        ce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n","        pt = tf.exp(-ce)\n","        return tf.reduce_mean(self.alpha * (1-pt)**self.gamma * ce)\n","\n","# Initialize model\n","model = create_enhanced_model()\n","\n","# Compile\n","model.compile(\n","    optimizer=optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5),\n","    loss=FocalLoss(),\n","    metrics=['accuracy']\n",")\n","\n","# Class weights for imbalanced data\n","class_counts = np.bincount(train_gen.classes)\n","total = len(train_gen.classes)\n","class_weights = {i: total/(len(class_counts)*count) for i, count in enumerate(class_counts)}\n","\n","# Callbacks\n","callbacks = [\n","    EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True),\n","    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n","    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy')\n","]\n","\n","# First training phase - train with frozen layers\n","print(\"Phase 1: Training with frozen backbone...\")\n","history1 = model.fit(\n","    train_gen,\n","    validation_data=test_gen,\n","    epochs=15,\n","    class_weight=class_weights,\n","    callbacks=callbacks\n",")\n","\n","# Unfreeze layers for fine-tuning\n","# print(\"Unfreezing EfficientNet layers for fine-tuning...\")\n","# for layer in model.layers:\n","#     if isinstance(layer, tf.keras.applications.EfficientNetB2):\n","#         for i, l in enumerate(layer.layers):\n","#             if i > 300:  # Unfreeze the last layers\n","#                 l.trainable = True\n","\n","# print(\"Unfreezing DenseNet layers for fine-tuning...\")\n","# for layer in model.layers:\n","#     if isinstance(layer, tf.keras.applications.DenseNet121):\n","#         for i, l in enumerate(layer.layers):\n","#             if i > 300:  # Unfreeze the last layers\n","#                 l.trainable = True\n","\n","# Unfreeze layers for fine-tuning\n","print(\"Unfreezing EfficientNet layers for fine-tuning...\")\n","for layer in model.layers:\n","    if layer.name.startswith('efficientnetb2'):  # Check by name prefix\n","        for i, l in enumerate(layer.layers):\n","            if i > 300:  # Unfreeze the last layers\n","                l.trainable = True\n","\n","print(\"Unfreezing DenseNet layers for fine-tuning...\")\n","for layer in model.layers:\n","    if layer.name.startswith('densenet121'):  # Check by name prefix\n","        for i, l in enumerate(layer.layers):\n","            if i > 300:  # Unfreeze the last layers\n","                l.trainable = True\n","\n","# Recompile with lower learning rate\n","model.compile(\n","    optimizer=optimizers.AdamW(learning_rate=5e-5, weight_decay=1e-6),\n","    loss=FocalLoss(),\n","    metrics=['accuracy']\n",")\n","\n","# Second training phase - fine-tune\n","print(\"Phase 2: Fine-tuning...\")\n","history2 = model.fit(\n","    train_gen,\n","    validation_data=test_gen,\n","    epochs=15,\n","    class_weight=class_weights,\n","    callbacks=callbacks,\n","    initial_epoch=history1.epoch[-1] + 1\n",")\n","\n","# Load best model and evaluate\n","try:\n","    model.load_weights('best_model.keras')\n","except:\n","    # Fall back to HDF5 format if needed\n","    model.save('best_model.h5')\n","    model = tf.keras.models.load_model('best_model.h5',\n","                                       custom_objects={'FocalLoss': FocalLoss,\n","                                                      'SelfAttention': SelfAttention})\n","\n","test_loss, test_acc = model.evaluate(test_gen)\n","print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n","print(f\"Final Test Loss: {test_loss:.4f}\")\n","\n","# If you want to save the final model\n","model.save('tomato_classifier_final.keras')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOKkzzPFLZbkUa7dbkEGXt5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}