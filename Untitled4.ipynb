{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"10o7nZ90iXSp5vTZ6w6E2mN7RyT4BYVWg","authorship_tag":"ABX9TyPESxgK/D+zE25WIB4bJYtE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a5472b69d792405180a8e6282257492f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf411e481f4249339ff28d26ac0e42a5","IPY_MODEL_876fa215ee6743a5b5489d50ca3bb8ee","IPY_MODEL_8afc2fbde76042bfa8fd8fbb525a7c80"],"layout":"IPY_MODEL_91d490ccb5c040a1b766d5fa1e0a8ab1"}},"cf411e481f4249339ff28d26ac0e42a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3350294fa6b6431bb6d2bd043a4448fd","placeholder":"​","style":"IPY_MODEL_a4b4dc47718e4287bf838f70f2df2d5a","value":"config.json: 100%"}},"876fa215ee6743a5b5489d50ca3bb8ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fa02f05b33a41b2944e3cf3924d8649","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f8860fbd63b436bba5fe98bd4ac5ecd","value":502}},"8afc2fbde76042bfa8fd8fbb525a7c80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f81b9a3afd040969fbf47e3a84c7f2b","placeholder":"​","style":"IPY_MODEL_0068212bf91b430bbaa7f56aa9cb26be","value":" 502/502 [00:00&lt;00:00, 49.0kB/s]"}},"91d490ccb5c040a1b766d5fa1e0a8ab1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3350294fa6b6431bb6d2bd043a4448fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4b4dc47718e4287bf838f70f2df2d5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fa02f05b33a41b2944e3cf3924d8649":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f8860fbd63b436bba5fe98bd4ac5ecd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f81b9a3afd040969fbf47e3a84c7f2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0068212bf91b430bbaa7f56aa9cb26be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3758490a074843919ffdc5646e3524e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aee48efa95a24f8a8359bdbf0f92b70d","IPY_MODEL_99daff93e1224b35b9d957003188c3f4","IPY_MODEL_0bda31eaf92d4186b12569c206db523b"],"layout":"IPY_MODEL_b858120d905f4c5a9a0c44bce8184255"}},"aee48efa95a24f8a8359bdbf0f92b70d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1a8d331389a4f188f31220b5109b8b4","placeholder":"​","style":"IPY_MODEL_8513e2161b7641e383744f30b3c8496d","value":"model.safetensors: 100%"}},"99daff93e1224b35b9d957003188c3f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d148c8e77c549328fa199e9be9d29d1","max":345579424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7631fffbfd534cc5b360f83a6b3b9271","value":345579424}},"0bda31eaf92d4186b12569c206db523b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51c417434719476983ef48cabaf7ea0b","placeholder":"​","style":"IPY_MODEL_b8c075d29e1a407da4abeffd2a340b00","value":" 346M/346M [00:01&lt;00:00, 269MB/s]"}},"b858120d905f4c5a9a0c44bce8184255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1a8d331389a4f188f31220b5109b8b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8513e2161b7641e383744f30b3c8496d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d148c8e77c549328fa199e9be9d29d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7631fffbfd534cc5b360f83a6b3b9271":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51c417434719476983ef48cabaf7ea0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8c075d29e1a407da4abeffd2a340b00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQaKZMeW9493","executionInfo":{"status":"ok","timestamp":1745460493818,"user_tz":-330,"elapsed":17802,"user":{"displayName":"Sumit","userId":"00621018113257149390"}},"outputId":"c3038425-891f-4748-d9c0-2f800767f733"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","zip_path = \"/content/drive/MyDrive/tomato_dataset/Tomato_5_class.zip\"\n","extract_path = \"/content/tomato_dataset\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n"],"metadata":{"id":"chgSduvwAiDV","executionInfo":{"status":"ok","timestamp":1745460662594,"user_tz":-330,"elapsed":5185,"user":{"displayName":"Sumit","userId":"00621018113257149390"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["vgg 16\n"],"metadata":{"id":"KwhxDW7YIsRC"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import DataLoader\n","import time\n"],"metadata":{"id":"HMLX8OWVJu_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Paths\n","train_dir = '/content/tomato_dataset/Tomato_5_class/Training_set'\n","test_dir = '/content/tomato_dataset/Tomato_5_class/Testing_set'\n","\n","# Transforms\n","transform_train = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","# Load Datasets\n","train_data = datasets.ImageFolder(train_dir, transform=transform_train)\n","test_data = datasets.ImageFolder(test_dir, transform=transform_test)\n","\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n","\n","# Load pretrained VGG16\n","vgg16 = models.vgg16(pretrained=True)\n","\n","# Freeze feature layers\n","for param in vgg16.features.parameters():\n","    param.requires_grad = False\n","\n","# Modify the classifier\n","vgg16.classifier[6] = nn.Linear(4096, 5)  # 5 classes\n","vgg16 = vgg16.to(device)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(vgg16.parameters(), lr=0.0001)\n","\n","# Training loop\n","epochs = 10\n","best_acc = 0.0\n","\n","for epoch in range(epochs):\n","    vgg16.train()\n","    running_loss = 0.0\n","\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = vgg16(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    # Evaluation\n","    vgg16.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = vgg16(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    acc = 100 * correct / total\n","    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {running_loss:.4f}, Test Accuracy: {acc:.2f}%\")\n","\n","    # Save best model\n","    if acc > best_acc:\n","        best_acc = acc\n","        torch.save(vgg16.state_dict(), \"best_vgg16_tomato.pth\")\n","        print(\"✅ Saved Best Model\")\n","\n","print(\"Training Complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xBwJdMWHcu4","executionInfo":{"status":"ok","timestamp":1745404052173,"user_tz":-330,"elapsed":604718,"user":{"displayName":"Sumit","userId":"00621018113257149390"}},"outputId":"38a5dac6-b853-45c2-f52e-26c9606e57f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:03<00:00, 141MB/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10] - Loss: 161.6492, Test Accuracy: 63.37%\n","✅ Saved Best Model\n","Epoch [2/10] - Loss: 139.4079, Test Accuracy: 61.73%\n","Epoch [3/10] - Loss: 128.9651, Test Accuracy: 66.59%\n","✅ Saved Best Model\n","Epoch [4/10] - Loss: 121.2799, Test Accuracy: 68.31%\n","✅ Saved Best Model\n","Epoch [5/10] - Loss: 114.0194, Test Accuracy: 69.88%\n","✅ Saved Best Model\n","Epoch [6/10] - Loss: 116.2628, Test Accuracy: 71.14%\n","✅ Saved Best Model\n","Epoch [7/10] - Loss: 111.4199, Test Accuracy: 70.90%\n","Epoch [8/10] - Loss: 107.2008, Test Accuracy: 67.69%\n","Epoch [9/10] - Loss: 104.7532, Test Accuracy: 72.31%\n","✅ Saved Best Model\n","Epoch [10/10] - Loss: 101.1273, Test Accuracy: 72.08%\n","Training Complete.\n"]}]},{"cell_type":"markdown","source":["vgg 16\n","\n"],"metadata":{"id":"1TK3oFhTIr-q"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from sklearn.utils.class_weight import compute_class_weight\n","import numpy as np\n","\n","# Set seed\n","tf.random.set_seed(42)\n","\n","# Paths\n","# DATASET_PATH = \"/content/Tomato_5_class/Training_set\"\n","# TEST_DATASET_PATH = \"/content/Tomato_5_class/Testing_set\"\n","\n","DATASET_PATH = '/content/tomato_dataset/Tomato_5_class/Training_set'\n","TEST_DATASET_PATH = '/content/tomato_dataset/Tomato_5_class/Testing_set'\n","# Parameters\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","EPOCHS_STAGE1 = 10\n","EPOCHS_STAGE2 = 20\n","NUM_CLASSES = 5\n","\n","# Data Generators\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.2\n",")\n","\n","val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    DATASET_PATH,\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='sparse',\n","    subset='training',\n","    shuffle=True\n",")\n","\n","validation_generator = val_datagen.flow_from_directory(\n","    DATASET_PATH,\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='sparse',\n","    subset='validation',\n","    shuffle=False\n",")\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","test_generator = test_datagen.flow_from_directory(\n","    TEST_DATASET_PATH,\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='sparse',\n","    shuffle=False\n",")\n","\n","# Compute class weights\n","labels = train_generator.classes\n","class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(labels),\n","    y=labels\n",")\n","class_weights_dict = dict(enumerate(class_weights))\n","print(\"Class Weights:\", class_weights_dict)\n","\n","# Callbacks\n","checkpoint_cb = ModelCheckpoint(\"best_vgg16_model.h5\", save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n","earlystop_cb = EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy', mode='max', verbose=1)\n","lr_schedule_cb = ReduceLROnPlateau(factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n","\n","# Load pretrained VGG16 model\n","base_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n","base_model.trainable = False  # Freeze feature layers\n","\n","# Custom head\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(256, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","output = Dense(NUM_CLASSES, activation='softmax')(x)\n","\n","model = Model(inputs=base_model.input, outputs=output)\n","\n","# Compile\n","model.compile(optimizer=Adam(learning_rate=1e-4),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Stage 1 Training (Frozen base)\n","history_stage1 = model.fit(\n","    train_generator,\n","    validation_data=validation_generator,\n","    epochs=EPOCHS_STAGE1,\n","    class_weight=class_weights_dict,\n","    callbacks=[checkpoint_cb, earlystop_cb, lr_schedule_cb]\n",")\n","\n","# Unfreeze top layers for fine-tuning\n","base_model.trainable = True\n","for layer in base_model.layers[:-4]:  # Freeze all but last 4 conv layers\n","    layer.trainable = False\n","\n","# Recompile with lower LR\n","model.compile(optimizer=Adam(learning_rate=1e-5),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Stage 2 Training (Fine-tuning)\n","history_stage2 = model.fit(\n","    train_generator,\n","    validation_data=validation_generator,\n","    epochs=EPOCHS_STAGE2,\n","    initial_epoch=history_stage1.epoch[-1] + 1,\n","    class_weight=class_weights_dict,\n","    callbacks=[checkpoint_cb, earlystop_cb, lr_schedule_cb]\n",")\n","\n","# Load the best saved model\n","best_model = tf.keras.models.load_model(\"best_vgg16_model.h5\")\n","\n","# Evaluate on test set\n","loss, accuracy = best_model.evaluate(test_generator)\n","print(f\"✅ Final Test Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwEigw9tLqwN","executionInfo":{"status":"ok","timestamp":1745405630870,"user_tz":-330,"elapsed":98421,"user":{"displayName":"Sumit","userId":"00621018113257149390"}},"outputId":"30bcb51e-8675-499d-b342-ea3c2294c25a"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Found 4100 images belonging to 5 classes.\n","Found 1022 images belonging to 5 classes.\n","Found 1275 images belonging to 5 classes.\n","Class Weights: {0: np.float64(0.38317757009345793), 1: np.float64(1.6734693877551021), 2: np.float64(1.6734693877551021), 3: np.float64(1.6734693877551021), 4: np.float64(1.6734693877551021)}\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470ms/step - accuracy: 0.2527 - loss: 1.7956\n","Epoch 1: val_accuracy improved from -inf to 0.19863, saving model to best_vgg16_model.h5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 620ms/step - accuracy: 0.2526 - loss: 1.7952 - val_accuracy: 0.1986 - val_loss: 1.5662 - learning_rate: 1.0000e-04\n","Epoch 2/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - accuracy: 0.2331 - loss: 1.6362\n","Epoch 2: val_accuracy did not improve from 0.19863\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 477ms/step - accuracy: 0.2331 - loss: 1.6363 - val_accuracy: 0.1575 - val_loss: 1.5975 - learning_rate: 1.0000e-04\n","Epoch 3/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.2387 - loss: 1.6176\n","Epoch 3: val_accuracy did not improve from 0.19863\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 483ms/step - accuracy: 0.2388 - loss: 1.6175 - val_accuracy: 0.1918 - val_loss: 1.5393 - learning_rate: 1.0000e-04\n","Epoch 4/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - accuracy: 0.3044 - loss: 1.5383\n","Epoch 4: val_accuracy improved from 0.19863 to 0.22016, saving model to best_vgg16_model.h5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 519ms/step - accuracy: 0.3043 - loss: 1.5385 - val_accuracy: 0.2202 - val_loss: 1.5162 - learning_rate: 1.0000e-04\n","Epoch 5/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - accuracy: 0.2989 - loss: 1.5384\n","Epoch 5: val_accuracy improved from 0.22016 to 0.44716, saving model to best_vgg16_model.h5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 480ms/step - accuracy: 0.2989 - loss: 1.5384 - val_accuracy: 0.4472 - val_loss: 1.4755 - learning_rate: 1.0000e-04\n","Epoch 6/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - accuracy: 0.3219 - loss: 1.5253\n","Epoch 6: val_accuracy improved from 0.44716 to 0.52055, saving model to best_vgg16_model.h5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 477ms/step - accuracy: 0.3219 - loss: 1.5252 - val_accuracy: 0.5205 - val_loss: 1.4417 - learning_rate: 1.0000e-04\n","Epoch 7/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - accuracy: 0.3824 - loss: 1.4964\n","Epoch 7: val_accuracy improved from 0.52055 to 0.54990, saving model to best_vgg16_model.h5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 474ms/step - accuracy: 0.3823 - loss: 1.4964 - val_accuracy: 0.5499 - val_loss: 1.4422 - learning_rate: 1.0000e-04\n","Epoch 8/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.3412 - loss: 1.5048\n","Epoch 8: val_accuracy did not improve from 0.54990\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 475ms/step - accuracy: 0.3413 - loss: 1.5047 - val_accuracy: 0.4207 - val_loss: 1.4210 - learning_rate: 1.0000e-04\n","Epoch 9/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - accuracy: 0.3999 - loss: 1.4691\n","Epoch 9: val_accuracy did not improve from 0.54990\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 470ms/step - accuracy: 0.3998 - loss: 1.4691 - val_accuracy: 0.5059 - val_loss: 1.4212 - learning_rate: 1.0000e-04\n","Epoch 10/10\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step - accuracy: 0.4056 - loss: 1.4520\n","Epoch 10: val_accuracy did not improve from 0.54990\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 486ms/step - accuracy: 0.4056 - loss: 1.4520 - val_accuracy: 0.4589 - val_loss: 1.4057 - learning_rate: 1.0000e-04\n","Restoring model weights from the end of the best epoch: 7.\n","Epoch 11/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - accuracy: 0.3948 - loss: 1.4757\n","Epoch 11: val_accuracy improved from 0.54990 to 0.58219, saving model to best_vgg16_model.h5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 540ms/step - accuracy: 0.3950 - loss: 1.4753 - val_accuracy: 0.5822 - val_loss: 1.2769 - learning_rate: 1.0000e-05\n","Epoch 12/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - accuracy: 0.4896 - loss: 1.3218\n","Epoch 12: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 489ms/step - accuracy: 0.4896 - loss: 1.3218 - val_accuracy: 0.3855 - val_loss: 1.3720 - learning_rate: 1.0000e-05\n","Epoch 13/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - accuracy: 0.4850 - loss: 1.3095\n","Epoch 13: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 499ms/step - accuracy: 0.4851 - loss: 1.3092 - val_accuracy: 0.5333 - val_loss: 1.1539 - learning_rate: 1.0000e-05\n","Epoch 14/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - accuracy: 0.5405 - loss: 1.2026\n","Epoch 14: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 491ms/step - accuracy: 0.5405 - loss: 1.2026 - val_accuracy: 0.4716 - val_loss: 1.1738 - learning_rate: 1.0000e-05\n","Epoch 15/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.5707 - loss: 1.1774\n","Epoch 15: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 483ms/step - accuracy: 0.5706 - loss: 1.1774 - val_accuracy: 0.4530 - val_loss: 1.1946 - learning_rate: 1.0000e-05\n","Epoch 16/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450ms/step - accuracy: 0.5410 - loss: 1.1517\n","Epoch 16: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 493ms/step - accuracy: 0.5410 - loss: 1.1517 - val_accuracy: 0.5010 - val_loss: 1.1070 - learning_rate: 1.0000e-05\n","Epoch 17/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step - accuracy: 0.5598 - loss: 1.1561\n","Epoch 17: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 496ms/step - accuracy: 0.5599 - loss: 1.1560 - val_accuracy: 0.4697 - val_loss: 1.1788 - learning_rate: 1.0000e-05\n","Epoch 18/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.5678 - loss: 1.1207\n","Epoch 18: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 481ms/step - accuracy: 0.5678 - loss: 1.1207 - val_accuracy: 0.4697 - val_loss: 1.1913 - learning_rate: 1.0000e-05\n","Epoch 19/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.5695 - loss: 1.1062\n","Epoch 19: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 482ms/step - accuracy: 0.5695 - loss: 1.1062 - val_accuracy: 0.4834 - val_loss: 1.1775 - learning_rate: 1.0000e-05\n","Epoch 20/20\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - accuracy: 0.5727 - loss: 1.0970\n","Epoch 20: val_accuracy did not improve from 0.58219\n","\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 498ms/step - accuracy: 0.5727 - loss: 1.0968 - val_accuracy: 0.4971 - val_loss: 1.1423 - learning_rate: 1.0000e-05\n","Restoring model weights from the end of the best epoch: 11.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446ms/step - accuracy: 0.8183 - loss: 1.0979\n","✅ Final Test Accuracy: 59.45%\n"]}]},{"cell_type":"code","source":["knn"],"metadata":{"id":"YFBWSqwljF-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Step 2: Set Paths to Dataset\n","# train_path = \"/content/Tomato_5_class/Tomato_5_class/Training_set\"\n","# test_path = \"/content/Tomato_5_class/Tomato_5_class/Testing_set\"\n","\n","train_path = '/content/tomato_dataset/Tomato_5_class/Training_set'\n","test_path = '/content/tomato_dataset/Tomato_5_class/Testing_set'\n","\n","# DATASET_PATH = '/content/tomato_dataset/Tomato_5_class/Training_set'\n","# TEST_DATASET_PATH = '/content/tomato_dataset/Tomato_5_class/Testing_set'\n","\n","# Step 3: Required Libraries\n","import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import Model\n","import os\n","\n","# Step 4: Image Generators\n","img_size = (224, 224)\n","batch_size = 32\n","\n","datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","train_generator = datagen.flow_from_directory(\n","    train_path,\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    shuffle=False\n",")\n","\n","test_generator = datagen.flow_from_directory(\n","    test_path,\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    shuffle=False\n",")\n","\n","# Step 5: Load Pretrained CNN (MobileNetV2)\n","base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n","model = Model(inputs=base_model.input, outputs=base_model.output)\n","\n","# Step 6: Feature Extraction Function\n","def extract_features(generator):\n","    features = model.predict(generator, verbose=1)\n","    labels = generator.classes\n","    return features, labels\n","\n","X_train, y_train = extract_features(train_generator)\n","X_test, y_test = extract_features(test_generator)\n","\n","# Step 7: KNN Classifier\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(X_train, y_train)\n","y_pred_knn = knn.predict(X_test)\n","\n","print(\"=== KNN Results ===\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n","print(classification_report(y_test, y_pred_knn))\n","\n","# Step 8: SVM Classifier\n","svm = SVC(kernel='rbf', C=1.0)\n","svm.fit(X_train, y_train)\n","y_pred_svm = svm.predict(X_test)\n","\n","print(\"=== SVM Results ===\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n","print(classification_report(y_test, y_pred_svm))\n","\n","# Step 9: Naive Bayes Classifier\n","nb = GaussianNB()\n","nb.fit(X_train, y_train)\n","y_pred_nb = nb.predict(X_test)\n","\n","print(\"=== Naive Bayes Results ===\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n","print(classification_report(y_test, y_pred_nb))\n","\n","# Step 10: Summary\n","print(\"\\n=== Summary ===\")\n","print(f\"KNN Accuracy:         {accuracy_score(y_test, y_pred_knn):.4f}\")\n","print(f\"SVM Accuracy:         {accuracy_score(y_test, y_pred_svm):.4f}\")\n","print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfZAR-kxjHXG","executionInfo":{"status":"ok","timestamp":1745460715471,"user_tz":-330,"elapsed":43881,"user":{"displayName":"Sumit","userId":"00621018113257149390"}},"outputId":"cfe8c251-a2a3-4fcf-b44e-ed60924fd794"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5122 images belonging to 5 classes.\n","Found 1275 images belonging to 5 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step\n","=== KNN Results ===\n","Accuracy: 0.7701960784313725\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.95      0.90       667\n","           1       0.50      0.50      0.50       152\n","           2       0.58      0.55      0.56       152\n","           3       0.65      0.55      0.60       152\n","           4       0.90      0.71      0.79       152\n","\n","    accuracy                           0.77      1275\n","   macro avg       0.70      0.65      0.67      1275\n","weighted avg       0.77      0.77      0.76      1275\n","\n","=== SVM Results ===\n","Accuracy: 0.6737254901960784\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.99      0.83       667\n","           1       0.57      0.05      0.10       152\n","           2       0.47      0.19      0.27       152\n","           3       0.43      0.41      0.42       152\n","           4       0.79      0.64      0.71       152\n","\n","    accuracy                           0.67      1275\n","   macro avg       0.59      0.46      0.47      1275\n","weighted avg       0.64      0.67      0.61      1275\n","\n","=== Naive Bayes Results ===\n","Accuracy: 0.35764705882352943\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.31      0.46       667\n","           1       0.17      0.80      0.28       152\n","           2       0.21      0.16      0.18       152\n","           3       0.40      0.24      0.30       152\n","           4       0.48      0.45      0.47       152\n","\n","    accuracy                           0.36      1275\n","   macro avg       0.44      0.39      0.34      1275\n","weighted avg       0.64      0.36      0.39      1275\n","\n","\n","=== Summary ===\n","KNN Accuracy:         0.7702\n","SVM Accuracy:         0.6737\n","Naive Bayes Accuracy: 0.3576\n"]}]},{"cell_type":"code","source":["# # ✅ Step 1: Mount Drive and Setup Paths\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# train_path = \"/content/Tomato_5_class/Tomato_5_class/Training_set\"\n","# test_path = \"/content/Tomato_5_class/Tomato_5_class/Testing_set\"\n","\n","# ✅ Step 2: Required Libraries\n","import numpy as np\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import Model\n","import os\n","\n","# ✅ Step 3: Setup Image Generators (with real-time data augmentation for underrepresented classes)\n","img_size = (224, 224)\n","batch_size = 32\n","\n","train_aug = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    rotation_range=20,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.1,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","train_generator = train_aug.flow_from_directory(\n","    train_path,\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    shuffle=False\n",")\n","\n","test_generator = test_gen.flow_from_directory(\n","    test_path,\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    shuffle=False\n",")\n","\n","# ✅ Step 4: Load Pretrained CNN (MobileNetV2 for feature extraction)\n","base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n","model = Model(inputs=base_model.input, outputs=base_model.output)\n","\n","# ✅ Step 5: Extract CNN Features\n","def extract_features(generator):\n","    features = model.predict(generator, verbose=1)\n","    labels = generator.classes\n","    return features, labels\n","\n","X_train, y_train = extract_features(train_generator)\n","X_test, y_test = extract_features(test_generator)\n","\n","# ✅ Step 6: Compute Class Weights for SVM\n","classes = np.unique(y_train)\n","class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n","class_weight_dict = dict(zip(classes, class_weights))\n","\n","# ✅ Step 7: KNN (Note: no weights, but we’ll monitor results)\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(X_train, y_train)\n","y_pred_knn = knn.predict(X_test)\n","\n","print(\"=== KNN Results ===\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n","print(classification_report(y_test, y_pred_knn))\n","\n","# ✅ Step 8: SVM (with class weights)\n","svm = SVC(kernel='rbf', C=1.0, class_weight=class_weight_dict)\n","svm.fit(X_train, y_train)\n","y_pred_svm = svm.predict(X_test)\n","\n","print(\"=== SVM Results ===\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n","print(classification_report(y_test, y_pred_svm))\n","\n","# ✅ Step 9: Naive Bayes (can’t use class weights directly, will try as-is)\n","nb = GaussianNB()\n","nb.fit(X_train, y_train)\n","y_pred_nb = nb.predict(X_test)\n","\n","print(\"=== Naive Bayes Results ===\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n","print(classification_report(y_test, y_pred_nb))\n","\n","# ✅ Step 10: Summary\n","print(\"\\n=== Summary ===\")\n","print(f\"KNN Accuracy:         {accuracy_score(y_test, y_pred_knn):.4f}\")\n","print(f\"SVM Accuracy:         {accuracy_score(y_test, y_pred_svm):.4f}\")\n","print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdpXCgCMjHVu","executionInfo":{"status":"ok","timestamp":1745461689384,"user_tz":-330,"elapsed":94518,"user":{"displayName":"Sumit","userId":"00621018113257149390"}},"outputId":"f0958c4c-158e-4670-a8b8-1f5836d32adb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5122 images belonging to 5 classes.\n","Found 1275 images belonging to 5 classes.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 397ms/step\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step\n","=== KNN Results ===\n","Accuracy: 0.6290196078431373\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.90      0.82       667\n","           1       0.27      0.28      0.27       152\n","           2       0.27      0.26      0.27       152\n","           3       0.51      0.25      0.33       152\n","           4       0.79      0.56      0.65       152\n","\n","    accuracy                           0.63      1275\n","   macro avg       0.52      0.45      0.47      1275\n","weighted avg       0.61      0.63      0.61      1275\n","\n","=== SVM Results ===\n","Accuracy: 0.5192156862745098\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.54      0.70       667\n","           1       0.22      0.68      0.33       152\n","           2       0.28      0.29      0.29       152\n","           3       0.40      0.39      0.39       152\n","           4       0.79      0.61      0.69       152\n","\n","    accuracy                           0.52      1275\n","   macro avg       0.53      0.50      0.48      1275\n","weighted avg       0.71      0.52      0.57      1275\n","\n","=== Naive Bayes Results ===\n","Accuracy: 0.3325490196078431\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.28      0.44       667\n","           1       0.16      0.88      0.27       152\n","           2       0.20      0.06      0.09       152\n","           3       0.37      0.24      0.29       152\n","           4       0.61      0.37      0.46       152\n","\n","    accuracy                           0.33      1275\n","   macro avg       0.46      0.37      0.31      1275\n","weighted avg       0.66      0.33      0.36      1275\n","\n","\n","=== Summary ===\n","KNN Accuracy:         0.6290\n","SVM Accuracy:         0.5192\n","Naive Bayes Accuracy: 0.3325\n"]}]},{"cell_type":"code","source":["# ✅ STEP 1: Mount Google Drive & Extract Dataset\n","from google.colab import drive\n","import zipfile\n","import os\n","\n","drive.mount('/content/drive')\n","\n","zip_path = \"/content/drive/MyDrive/Tomato Maturity Detection Dataset.zip\"\n","extract_path = \"/content/tomato_maturity\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","# ✅ STEP 2: Import Libraries\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.optim import AdamW\n","from transformers import ViTForImageClassification\n","from tqdm import tqdm\n","import numpy as np\n","import random\n","\n","# ✅ STEP 3: Set Seed and Device\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# ✅ STEP 4: Data Transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n","])\n","\n","# ✅ STEP 5: Load Dataset and Split\n","data_dir = os.path.join(extract_path, \"Tomato Maturity Detection Dataset\", \"Augment Dataset\")\n","full_dataset = ImageFolder(root=data_dir, transform=transform)\n","\n","train_size = int(0.7 * len(full_dataset))\n","val_size = int(0.15 * len(full_dataset))\n","test_size = len(full_dataset) - train_size - val_size\n","\n","train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# ✅ STEP 6: Define ViT Model for 2 Classes\n","model = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224-in21k\",\n","    num_labels=2  # Binary: MATURE, IMMATURE\n",")\n","model.to(device)\n","\n","# ✅ STEP 7: Training Config\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","best_val_acc = 0.0\n","save_path = \"best_vit_tomato_model.pth\"\n","\n","# ✅ STEP 8: Training and Evaluation Functions\n","def train_one_epoch(model, loader, optimizer, criterion):\n","    model.train()\n","    total_loss, correct, total = 0, 0, 0\n","\n","    for imgs, labels in tqdm(loader, desc=\"Training\"):\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        outputs = model(imgs).logits\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        preds = outputs.argmax(dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    return total_loss / len(loader), correct / total\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss, correct, total = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for imgs, labels in tqdm(loader, desc=\"Evaluating\"):\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            outputs = model(imgs).logits\n","            loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item()\n","            preds = outputs.argmax(dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","    return total_loss / len(loader), correct / total\n","\n","# ✅ STEP 9: Training Loop\n","EPOCHS = 5\n","for epoch in range(EPOCHS):\n","    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n","\n","    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n","    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n","\n","    val_loss, val_acc = evaluate(model, val_loader, criterion)\n","    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"✅ Best model saved with val acc: {val_acc:.4f}\")\n","\n","# ✅ STEP 10: Final Test\n","model.load_state_dict(torch.load(save_path))\n","model.eval()\n","\n","test_loss, test_acc = evaluate(model, test_loader, criterion)\n","print(f\"\\n📊 Final Test Accuracy: {test_acc:.4f} | Loss: {test_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":864,"referenced_widgets":["a5472b69d792405180a8e6282257492f","cf411e481f4249339ff28d26ac0e42a5","876fa215ee6743a5b5489d50ca3bb8ee","8afc2fbde76042bfa8fd8fbb525a7c80","91d490ccb5c040a1b766d5fa1e0a8ab1","3350294fa6b6431bb6d2bd043a4448fd","a4b4dc47718e4287bf838f70f2df2d5a","0fa02f05b33a41b2944e3cf3924d8649","5f8860fbd63b436bba5fe98bd4ac5ecd","0f81b9a3afd040969fbf47e3a84c7f2b","0068212bf91b430bbaa7f56aa9cb26be","3758490a074843919ffdc5646e3524e7","aee48efa95a24f8a8359bdbf0f92b70d","99daff93e1224b35b9d957003188c3f4","0bda31eaf92d4186b12569c206db523b","b858120d905f4c5a9a0c44bce8184255","d1a8d331389a4f188f31220b5109b8b4","8513e2161b7641e383744f30b3c8496d","1d148c8e77c549328fa199e9be9d29d1","7631fffbfd534cc5b360f83a6b3b9271","51c417434719476983ef48cabaf7ea0b","b8c075d29e1a407da4abeffd2a340b00"]},"id":"cAhPYCl0jHQV","executionInfo":{"status":"ok","timestamp":1745467981763,"user_tz":-330,"elapsed":2028286,"user":{"displayName":"Sumit","userId":"00621018113257149390"}},"outputId":"44bff985-249c-41aa-9b2d-b1951d89f29d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5472b69d792405180a8e6282257492f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3758490a074843919ffdc5646e3524e7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 88/88 [05:21<00:00,  3.66s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.1889 | Train Accuracy: 0.9832\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 19/19 [00:57<00:00,  3.02s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Loss: 0.0403 | Val Accuracy: 0.9983\n","✅ Best model saved with val acc: 0.9983\n","\n","Epoch 2/5\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 88/88 [05:20<00:00,  3.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0236 | Train Accuracy: 0.9993\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 19/19 [00:56<00:00,  2.97s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Loss: 0.0239 | Val Accuracy: 0.9983\n","\n","Epoch 3/5\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 88/88 [05:20<00:00,  3.65s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0204 | Train Accuracy: 0.9979\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 19/19 [00:55<00:00,  2.94s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Loss: 0.0200 | Val Accuracy: 0.9967\n","\n","Epoch 4/5\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 88/88 [05:19<00:00,  3.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0105 | Train Accuracy: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 19/19 [00:56<00:00,  2.98s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Loss: 0.0184 | Val Accuracy: 0.9967\n","\n","Epoch 5/5\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 88/88 [05:17<00:00,  3.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0082 | Train Accuracy: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 19/19 [00:57<00:00,  3.00s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Loss: 0.0171 | Val Accuracy: 0.9967\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 19/19 [00:56<00:00,  3.00s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","📊 Final Test Accuracy: 0.9983 | Loss: 0.0372\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# 🚩 Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from transformers import ViTForImageClassification\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import numpy as np\n","\n","# ✅ Config\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Paths\n","data_dir = \"/content/drive/MyDrive/Tomato Maturity Detection Dataset/Augment Dataset\"\n","save_path = \"/content/drive/MyDrive/2classbestmodel.pth\"\n","\n","# 🔁 Transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","])\n","\n","# 📦 Dataset & Dataloader\n","full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","class_names = full_dataset.classes  # ['IMMATURE', 'MATURE']\n","print(\"Classes:\", class_names)\n","\n","from torch.utils.data import random_split\n","train_size = int(0.8 * len(full_dataset))\n","val_size = len(full_dataset) - train_size\n","train_data, val_data = random_split(full_dataset, [train_size, val_size])\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n","\n","# ✅ Model Setup\n","model = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224-in21k\",\n","    num_labels=2  # 2 classes: MATURE / IMMATURE\n",")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 🔁 Training & Evaluation Functions\n","def train_one_epoch(model, loader, optimizer, criterion):\n","    model.train()\n","    total_loss, correct, total = 0, 0, 0\n","    for imgs, labels in tqdm(loader, desc=\"Training\"):\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        outputs = model(imgs).logits\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        preds = outputs.argmax(dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","    return total_loss / len(loader), correct / total\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss, correct, total = 0, 0, 0\n","    with torch.no_grad():\n","        for imgs, labels in tqdm(loader, desc=\"Evaluating\"):\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            outputs = model(imgs).logits\n","            loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item()\n","            preds = outputs.argmax(dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","    return total_loss / len(loader), correct / total\n","\n","# 🧠 Training Loop\n","EPOCHS = 5\n","train_losses, val_losses = [], []\n","train_accuracies, val_accuracies = [], []\n","best_val_acc = 0.0\n","\n","for epoch in range(EPOCHS):\n","    print(f\"\\n📚 Epoch {epoch+1}/{EPOCHS}\")\n","    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n","    val_loss, val_acc = evaluate(model, val_loader, criterion)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    train_accuracies.append(train_acc)\n","    val_accuracies.append(val_acc)\n","\n","    print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n","    print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"✅ Best model saved to Google Drive at: {save_path}\")\n","\n","# 📈 Plot Learning Curves\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_accuracies, label='Train Acc')\n","plt.plot(val_accuracies, label='Val Acc')\n","plt.title(\"Accuracy\")\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Val Loss')\n","plt.title(\"Loss\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"09nafM1CjHMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from transformers import ViTForImageClassification\n","from torchvision import transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Class labels\n","class_names = ['IMMATURE', 'MATURE']  # Binary\n","\n","# 🔁 Same transform as training\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","])\n","\n","# ✅ Load model\n","model = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224-in21k\",\n","    num_labels=2\n",")\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/2classbestmodel.pth\", map_location=device))\n","model.to(device)\n","model.eval()\n","\n","# 🔍 Predict top-5\n","def predict_top5(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    img_tensor = transform(image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(img_tensor).logits\n","        probs = F.softmax(outputs, dim=1)\n","        top5 = torch.topk(probs, k=2)\n","\n","    top5_probs = top5.values[0].cpu().numpy()\n","    top5_indices = top5.indices[0].cpu().numpy()\n","\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.title(\"Test Image\")\n","    plt.show()\n","\n","    print(\"\\n🔍 Top Predictions:\")\n","    for idx, prob in zip(top5_indices, top5_probs):\n","        print(f\"{class_names[idx]}: {prob:.4f}\")\n","\n","# 🧪 Test on alien image\n","predict_top5(\"/content/alien_tomato.jpg\")  # Change path as needed\n"],"metadata":{"id":"KRfez7L1jHK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8N0lyOa-jHG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pSralQTxjHFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CU_7PrdljGrB"},"execution_count":null,"outputs":[]}]}